#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è auto.ru - —Å–±–æ—Ä –º–∞—Ä–æ–∫ –∏ –º–æ–¥–µ–ª–µ–π –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π
"""

import requests
import csv
import time
import os
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import sys

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å Python –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –ª–æ–≥–≥–µ—Ä–∞
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from utils.logger import log

class AutoRuParser:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.base_url = "https://auto.ru"
        
    def get_page(self, url):
        """–ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        try:
            log.info(f"üìÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e:
            log.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
            return None
    
    def parse_brands_and_models(self, url, vehicle_type):
        """–ü–∞—Ä—Å–∏–º –º–∞—Ä–∫–∏ –∏ –º–æ–¥–µ–ª–∏ —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        html = self.get_page(url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        brands_data = []
        
        # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –º–∞—Ä–∫–∞–º–∏ (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ)
        brand_selectors = [
            'select[name="mark"] option',
            '.Select[data-ga-name="mark"] option',
            '[data-ftid="sales__filter_mark"] option',
            'select[data-ftid="sales__filter_mark"] option'
        ]
        
        brand_options = None
        for selector in brand_selectors:
            brand_options = soup.select(selector)
            if brand_options:
                log.info(f"‚úÖ –ù–∞—à–ª–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä –º–∞—Ä–æ–∫: {selector}")
                break
        
        if not brand_options:
            log.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–ø–∏—Å–æ–∫ –º–∞—Ä–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {url}")
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–∞—Ä–∫–∏ –¥—Ä—É–≥–∏–º —Å–ø–æ—Å–æ–±–æ–º
            brand_links = soup.select('a[href*="/cars/"]')
            log.info(f"üîó –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –º–∞—Ä–∫–∏: {len(brand_links)}")
            return []
        
        log.info(f"üè∑Ô∏è –ù–∞–π–¥–µ–Ω–æ –º–∞—Ä–æ–∫: {len(brand_links)}")
        
        # –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—É—é –º–∞—Ä–∫—É
        for option in brand_links:
            brand_name = option.get_text(strip=True)
            brand_value = option.get('value') or option.get('href', '')
            
            if not brand_name or brand_name in ['–õ—é–±–∞—è', '–í—Å–µ –º–∞—Ä–∫–∏', '']:
                continue
                
            log.info(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–∞—Ä–∫—É: {brand_name}")
            
            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è —ç—Ç–æ–π –º–∞—Ä–∫–∏
            models = self.get_models_for_brand(brand_name, brand_value, vehicle_type)
            
            for model_name in models:
                brands_data.append({
                    'brand': brand_name,
                    'model': model_name,
                    'vehicle_type': vehicle_type
                })
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å —Å–µ—Ä–≤–µ—Ä
            time.sleep(1)
        
        return brands_data
    
    def get_models_for_brand(self, brand_name, brand_value, vehicle_type):
        """–ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–∞—Ä–∫–∏"""
        models = []
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –º–æ–¥–µ–ª—è–º–∏
        if 'cars' in vehicle_type.lower():
            model_url = f"https://auto.ru/nizhniy_novgorod/cars/{brand_name.lower()}/all/"
        else:
            model_url = f"https://auto.ru/nizhniy_novgorod/lcv/{brand_name.lower()}/all/"
        
        html = self.get_page(model_url)
        if not html:
            return models
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ò—â–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä –º–æ–¥–µ–ª–µ–π
        model_selectors = [
            'select[name="model"] option',
            '.Select[data-ga-name="model"] option',
            '[data-ftid="sales__filter_model"] option',
            'select[data-ftid="sales__filter_model"] option'
        ]
        
        model_options = None
        for selector in model_selectors:
            model_options = soup.select(selector)
            if model_options:
                break
        
        if model_options:
            for option in model_options:
                model_name = option.get_text(strip=True)
                if model_name and model_name not in ['–õ—é–±–∞—è', '–í—Å–µ –º–æ–¥–µ–ª–∏', '']:
                    models.append(model_name)
        
        log.info(f"   üöó –ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è {brand_name}: {len(models)}")
        return models
    
    def save_to_csv(self, data, filename):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ CSV —Ñ–∞–π–ª"""
        desktop_path = os.path.join(os.path.expanduser("~"), "Desktop")
        filepath = os.path.join(desktop_path, filename)
        
        try:
            with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
                fieldnames = ['brand', 'model', 'vehicle_type']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                
                writer.writeheader()
                for row in data:
                    writer.writerow(row)
            
            log.info(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {filepath}")
            log.info(f"üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(data)}")
            return True
            
        except Exception as e:
            log.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ CSV: {e}")
            return False
    
    def run_parser(self):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
        log.info("üöó –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–µ—Ä Auto.ru...")
        
        all_data = []
        
        # –ü–∞—Ä—Å–∏–º –ª–µ–≥–∫–æ–≤—ã–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏
        log.info("üîç –ü–∞—Ä—Å–∏–º —Ä–∞–∑–¥–µ–ª '–õ–µ–≥–∫–æ–≤—ã–µ –∞–≤—Ç–æ'...")
        cars_url = "https://auto.ru/nizhniy_novgorod/cars/all/"
        cars_data = self.parse_brands_and_models(cars_url, "–õ–µ–≥–∫–æ–≤–æ–π")
        all_data.extend(cars_data)
        
        # –ü–∞—Ä—Å–∏–º –ª–µ–≥–∫–∏–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –∞–≤—Ç–æ–º–æ–±–∏–ª–∏
        log.info("üîç –ü–∞—Ä—Å–∏–º —Ä–∞–∑–¥–µ–ª '–õ—ë–≥–∫–∏–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –∞–≤—Ç–æ'...")
        lcv_url = "https://auto.ru/nizhniy_novgorod/lcv/all/"
        lcv_data = self.parse_brands_and_models(lcv_url, "–ì—Ä—É–∑–æ–≤–æ–π")
        all_data.extend(lcv_data)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if all_data:
            self.save_to_csv(all_data, "auto_ru_brands_models.csv")
            log.info("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        else:
            log.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ")
        
        return all_data

def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞"""
    try:
        parser = AutoRuParser()
        parser.run_parser()
        
    except Exception as e:
        log.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()
