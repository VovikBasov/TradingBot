#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ (—à–∞–≥ 3 —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞—Ä–±–∏—Ç—Ä–∞–∂–∞).
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É —Ü–µ–Ω –∏ –Ω–∞—Ö–æ–¥–∏—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ä—ã –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏.
"""

import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Dict, Tuple, Set
import warnings
warnings.filterwarnings('ignore')

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

class CorrelationCalculator:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏"""
    
    def __init__(self, min_correlation: float = 0.7, min_common_days: int = 100):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        
        Args:
            min_correlation: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –¥–ª—è –æ—Ç–±–æ—Ä–∞ –ø–∞—Ä
            min_common_days: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö —Ç–æ—Ä–≥–æ–≤—ã—Ö –¥–Ω–µ–π
        """
        self.min_correlation = min_correlation
        self.min_common_days = min_common_days
        self.price_matrix = None
        self.correlation_matrix = None
        self.strong_pairs = []
        
        print(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CorrelationCalculator")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {min_correlation}")
        print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –æ–±—â–∏—Ö –¥–Ω–µ–π: {min_common_days}")
    
    def find_latest_price_file(self) -> Optional[Path]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π CSV —Ñ–∞–π–ª —Å –º–∞—Ç—Ä–∏—Ü–µ–π —Ü–µ–Ω"""
        project_root = Path.cwd()
        price_files = list(project_root.glob("historical_prices_*.csv"))
        
        if not price_files:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª —Å –º–∞—Ç—Ä–∏—Ü–µ–π —Ü–µ–Ω (historical_prices_*.csv)")
            print("   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python src/data_feed/fetch_historical_prices.py")
            return None
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
        price_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        latest_file = price_files[0]
        
        print(f"üìÅ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª —Å –º–∞—Ç—Ä–∏—Ü–µ–π —Ü–µ–Ω: {latest_file}")
        print(f"   –†–∞–∑–º–µ—Ä: {latest_file.stat().st_size / 1024:.1f} –ö–ë")
        
        return latest_file
    
    def find_latest_metadata_file(self) -> Optional[Path]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π CSV —Ñ–∞–π–ª —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        project_root = Path.cwd()
        metadata_files = list(project_root.glob("historical_metadata_*.csv"))
        
        if not metadata_files:
            print("‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (historical_metadata_*.csv)")
            return None
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
        metadata_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        latest_file = metadata_files[0]
        
        print(f"üìÅ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏: {latest_file}")
        return latest_file
    
    def load_price_matrix(self, filepath: Path) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É —Ü–µ–Ω –∏–∑ CSV —Ñ–∞–π–ª–∞"""
        try:
            # –ß–∏—Ç–∞–µ–º CSV, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç—É –∫–∞–∫ –∏–Ω–¥–µ–∫—Å
            df = pd.read_csv(filepath, index_col='date', parse_dates=True)
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ —Ü–µ–Ω: {df.shape[0]} –¥–Ω–µ–π √ó {df.shape[1]} –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤")
            print(f"   –ü–µ—Ä–∏–æ–¥: {df.index.min().date()} - {df.index.max().date()}")
            
            return df
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {filepath}: {e}")
            sys.exit(1)
    
    def load_metadata(self, filepath: Optional[Path]) -> Optional[pd.DataFrame]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
        if filepath is None:
            return None
            
        try:
            df = pd.read_csv(filepath)
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {len(df)} –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤")
            return df
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
            return None
    
    def calculate_correlation_matrix(self, price_matrix: pd.DataFrame) -> pd.DataFrame:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –≤—Å–µ–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏"""
        print(f"\nüìä –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π...")
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        correlation_matrix = price_matrix.corr()
        
        print(f"‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞")
        print(f"   –†–∞–∑–º–µ—Ä: {correlation_matrix.shape[0]} √ó {correlation_matrix.shape[1]}")
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        corr_values = correlation_matrix.values.flatten()
        corr_values = corr_values[~np.isnan(corr_values)]  # –£–±–∏—Ä–∞–µ–º NaN
        corr_values = corr_values[corr_values != 1.0]  # –£–±–∏—Ä–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Å —Å–æ–±–æ–π
        
        if len(corr_values) > 0:
            print(f"   –°—Ä–µ–¥–Ω—è—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {np.mean(corr_values):.3f}")
            print(f"   –ú–µ–¥–∏–∞–Ω–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {np.median(corr_values):.3f}")
            print(f"   –ú–∞–∫—Å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {np.max(corr_values):.3f}")
            print(f"   –ú–∏–Ω –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {np.min(corr_values):.3f}")
        
        return correlation_matrix
    
    def find_strong_pairs(self, price_matrix: pd.DataFrame, correlation_matrix: pd.DataFrame) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø–∞—Ä—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
        print(f"\nüîç –ò—â–µ–º —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø–∞—Ä—ã (–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è > {self.min_correlation})...")
        
        pairs = []
        tickers = correlation_matrix.columns
        corr_values = correlation_matrix.values
        
        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –ø–∞—Ä–∞–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        for i in range(len(tickers)):
            for j in range(i + 1, len(tickers)):
                corr = corr_values[i, j]
                
                if not np.isnan(corr) and abs(corr) >= self.min_correlation:
                    ticker1 = tickers[i]
                    ticker2 = tickers[j]
                    
                    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —ç—Ç–∏–º —Ç–∏–∫–µ—Ä–∞–º
                    data1 = price_matrix[ticker1].dropna()
                    data2 = price_matrix[ticker2].dropna()
                    
                    # –ù–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ –¥–∞—Ç—ã
                    common_dates = data1.index.intersection(data2.index)
                    common_days = len(common_dates)
                    
                    if common_days >= self.min_common_days:
                        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                        pair_data = {
                            'ticker1': ticker1,
                            'ticker2': ticker2,
                            'correlation': corr,
                            'abs_correlation': abs(corr),
                            'common_days': common_days,
                            'price_ratio': data1.mean() / data2.mean() if data2.mean() != 0 else 0,
                            'volatility_ratio': data1.std() / data2.std() if data2.std() != 0 else 0,
                            'ticker1_mean_price': data1.mean(),
                            'ticker2_mean_price': data2.mean(),
                            'ticker1_volatility': data1.std(),
                            'ticker2_volatility': data2.std()
                        }
                        
                        pairs.append(pair_data)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        pairs.sort(key=lambda x: x['abs_correlation'], reverse=True)
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(pairs)} –ø–∞—Ä —Å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π > {self.min_correlation}")
        
        return pairs
    
    def analyze_pair_characteristics(self, pairs: List[Dict], metadata: Optional[pd.DataFrame]) -> pd.DataFrame:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø–∞—Ä –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        if not pairs:
            return pd.DataFrame()
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
        pairs_df = pd.DataFrame(pairs)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –µ—Å—Ç—å
        if metadata is not None:
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            metadata_dict = metadata.set_index('ticker').to_dict('index')
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –ø–µ—Ä–≤–æ–º—É —Ç–∏–∫–µ—Ä—É
            for field in ['type', 'currency', 'name']:
                if field in metadata_dict.get(next(iter(metadata_dict.keys())), {}):
                    pairs_df[f'ticker1_{field}'] = pairs_df['ticker1'].map(
                        lambda x: metadata_dict.get(x, {}).get(field, 'N/A')
                    )
                    pairs_df[f'ticker2_{field}'] = pairs_df['ticker2'].map(
                        lambda x: metadata_dict.get(x, {}).get(field, 'N/A')
                    )
        
        return pairs_df
    
    def calculate_cointegration(self, price_matrix: pd.DataFrame, pairs_df: pd.DataFrame, top_n: int = 20) -> pd.DataFrame:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –¥–ª—è —Ç–æ–ø-N –ø–∞—Ä.
        –ö–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤–∞–∂–Ω–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞—Ä–±–∏—Ç—Ä–∞–∂–∞.
        """
        if pairs_df.empty:
            return pairs_df
        
        print(f"\nüìà –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –¥–ª—è —Ç–æ–ø-{min(top_n, len(pairs_df))} –ø–∞—Ä...")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        pairs_to_test = pairs_df.head(top_n).copy()
        
        for idx, row in pairs_to_test.iterrows():
            ticker1 = row['ticker1']
            ticker2 = row['ticker2']
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            series1 = price_matrix[ticker1].dropna()
            series2 = price_matrix[ticker2].dropna()
            
            # –ù–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ –¥–∞—Ç—ã
            common_idx = series1.index.intersection(series2.index)
            if len(common_idx) < 50:  # –ù—É–∂–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∞
                pairs_to_test.loc[idx, 'cointegration_pvalue'] = np.nan
                pairs_to_test.loc[idx, 'cointegration_score'] = np.nan
                continue
            
            # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            series1_aligned = series1.loc[common_idx]
            series2_aligned = series2.loc[common_idx]
            
            try:
                # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –Ω–∞ –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
                # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å statsmodels.tsa.stattools.coint
                # –ó–¥–µ—Å—å –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ - –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –æ—Å—Ç–∞—Ç–∫–æ–≤
                
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–ø—Ä–µ–¥
                spread = series1_aligned - series2_aligned
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å —Å–ø—Ä–µ–¥–∞ —á–µ—Ä–µ–∑ –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é
                # (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∑–∞–º–µ–Ω–∏—Ç–µ–ª—å —Ç–µ—Å—Ç–∞ –î–∏–∫–∏-–§—É–ª–ª–µ—Ä–∞)
                from scipy import stats
                
                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –ø–µ—Ä–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
                if len(spread) > 1:
                    autocorr = spread.autocorr(lag=1)
                    # –ß–µ–º –±–ª–∏–∂–µ autocorr –∫ 0, —Ç–µ–º –±–æ–ª–µ–µ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã–π —Ä—è–¥
                    pairs_to_test.loc[idx, 'cointegration_score'] = 1 - abs(autocorr)
                else:
                    pairs_to_test.loc[idx, 'cointegration_score'] = np.nan
                
                # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ p-value (—É—Å–ª–æ–≤–Ω–∞—è)
                pairs_to_test.loc[idx, 'cointegration_pvalue'] = 0.05 if abs(autocorr) < 0.3 else 0.5
                
            except Exception as e:
                pairs_to_test.loc[idx, 'cointegration_pvalue'] = np.nan
                pairs_to_test.loc[idx, 'cointegration_score'] = np.nan
        
        print(f"‚úÖ –ö–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞ –¥–ª—è {len(pairs_to_test)} –ø–∞—Ä")
        
        return pairs_to_test
    
    def save_results(self, pairs_df: pd.DataFrame, correlation_matrix: pd.DataFrame, price_matrix: pd.DataFrame):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–∞—Ä—ã
        if not pairs_df.empty:
            pairs_file = f"correlation_pairs_{timestamp}.csv"
            pairs_df.to_csv(pairs_file, index=False)
            print(f"üíæ –ü–∞—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {pairs_file}")
            print(f"   –í—Å–µ–≥–æ –ø–∞—Ä: {len(pairs_df)}")
        
        # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        corr_matrix_file = f"correlation_matrix_{timestamp}.csv"
        correlation_matrix.to_csv(corr_matrix_file)
        print(f"üíæ –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {corr_matrix_file}")
        
        # 3. –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        if not pairs_df.empty:
            recommendations_file = f"trading_recommendations_{timestamp}.txt"
            with open(recommendations_file, 'w') as f:
                f.write("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ü–ê–†–ê–ú –î–õ–Ø –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–û–ì–û –ê–†–ë–ò–¢–†–ê–ñ–ê\n")
                f.write("=" * 70 + "\n\n")
                
                f.write(f"–í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤: {price_matrix.shape[1]}\n")
                f.write(f"–ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞: {price_matrix.index.min().date()} - {price_matrix.index.max().date()}\n")
                f.write(f"–¢–æ—Ä–≥–æ–≤—ã—Ö –¥–Ω–µ–π: {price_matrix.shape[0]}\n")
                f.write(f"–ù–∞–π–¥–µ–Ω–æ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏—Ö –ø–∞—Ä: {len(pairs_df)}\n\n")
                
                f.write("–¢–û–ü-10 –ü–ê–† –î–õ–Ø –î–ê–õ–¨–ù–ï–ô–®–ï–ì–û –ê–ù–ê–õ–ò–ó–ê:\n")
                f.write("-" * 70 + "\n")
                
                for i, (_, row) in enumerate(pairs_df.head(10).iterrows(), 1):
                    f.write(f"\n{i}. {row['ticker1']} ‚Üî {row['ticker2']}\n")
                    f.write(f"   –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {row['correlation']:.3f}\n")
                    f.write(f"   –û–±—â–∏—Ö –¥–Ω–µ–π: {row['common_days']}\n")
                    if 'cointegration_score' in row and not pd.isna(row['cointegration_score']):
                        f.write(f"   –û—Ü–µ–Ω–∫–∞ –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏: {row['cointegration_score']:.3f}\n")
                    f.write(f"   –°—Ä–µ–¥–Ω–∏–µ —Ü–µ–Ω—ã: {row['ticker1_mean_price']:.2f} / {row['ticker2_mean_price']:.2f}\n")
                    f.write(f"   –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å: {row['ticker1_volatility']:.3f} / {row['ticker2_volatility']:.3f}\n")
            
            print(f"üíæ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {recommendations_file}")
        
        # 4. –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        stats_file = f"correlation_stats_{timestamp}.txt"
        with open(stats_file, 'w') as f:
            f.write("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–û–†–†–ï–õ–Ø–¶–ò–û–ù–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê\n")
            f.write("=" * 50 + "\n")
            
            stats = {
                'timestamp': timestamp,
                'total_instruments': price_matrix.shape[1],
                'trading_days': price_matrix.shape[0],
                'strong_pairs_found': len(pairs_df),
                'min_correlation_threshold': self.min_correlation,
                'min_common_days': self.min_common_days,
                'pairs_file': pairs_file if not pairs_df.empty else None,
                'correlation_matrix_file': corr_matrix_file,
                'recommendations_file': recommendations_file if not pairs_df.empty else None
            }
            
            for key, value in stats.items():
                f.write(f"{key:30}: {value}\n")
        
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {stats_file}")
        
        return {
            'pairs_file': pairs_file if not pairs_df.empty else None,
            'correlation_matrix_file': corr_matrix_file,
            'recommendations_file': recommendations_file if not pairs_df.empty else None,
            'stats_file': stats_file
        }
    
    def print_summary(self, pairs_df: pd.DataFrame, files: dict):
        """–í—ã–≤–æ–¥–∏—Ç –∏—Ç–æ–≥–æ–≤—É—é —Å–≤–æ–¥–∫—É"""
        print("\n" + "=" * 70)
        print("üéØ –ò–¢–û–ì–ò –ö–û–†–†–ï–õ–Ø–¶–ò–û–ù–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê")
        print("=" * 70)
        
        if not pairs_df.empty:
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(pairs_df)} —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏—Ö –ø–∞—Ä")
            
            print(f"\nüèÜ –¢–û–ü-5 –ø–∞—Ä –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏:")
            for i, (_, row) in enumerate(pairs_df.head(5).iterrows(), 1):
                print(f"   {i}. {row['ticker1']} ‚Üî {row['ticker2']}: {row['correlation']:.3f}")
                if 'ticker1_type' in row and 'ticker2_type' in row:
                    print(f"      –¢–∏–ø—ã: {row['ticker1_type']} / {row['ticker2_type']}")
                print(f"      –û–±—â–∏—Ö –¥–Ω–µ–π: {row['common_days']}, –¶–µ–Ω—ã: {row['ticker1_mean_price']:.2f}/{row['ticker2_mean_price']:.2f}")
            
            # –ê–Ω–∞–ª–∏–∑ –ø–æ —Ç–∏–ø–∞–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
            if 'ticker1_type' in pairs_df.columns and 'ticker2_type' in pairs_df.columns:
                print(f"\nüìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º –ø–∞—Ä:")
                type_pairs = pairs_df.apply(
                    lambda x: f"{x['ticker1_type']}-{x['ticker2_type']}", axis=1
                )
                for pair_type, count in type_pairs.value_counts().head(5).items():
                    print(f"   {pair_type:15}: {count} –ø–∞—Ä")
        
        print(f"\nüíæ –§–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        for key, value in files.items():
            if value:
                print(f"   {key:25}: {value}")
        
        print(f"\nüéØ –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: –ê–Ω–∞–ª–∏–∑ —Å–ø—Ä–µ–¥–∞ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –ø–∞—Ä–∞–º")
        print("=" * 70)
    
    def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞"""
        print("=" * 70)
        print("üîó –†–ê–°–ß–Å–¢ –ö–û–†–†–ï–õ–Ø–¶–ò–ô –î–õ–Ø –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–û–ì–û –ê–†–ë–ò–¢–†–ê–ñ–ê")
        print("=" * 70)
        
        # 1. –ù–∞—Ö–æ–¥–∏–º —Ñ–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏
        price_file = self.find_latest_price_file()
        if not price_file:
            return
        
        metadata_file = self.find_latest_metadata_file()
        
        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        price_matrix = self.load_price_matrix(price_file)
        metadata = self.load_metadata(metadata_file)
        
        # 3. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        correlation_matrix = self.calculate_correlation_matrix(price_matrix)
        
        # 4. –ù–∞—Ö–æ–¥–∏–º —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø–∞—Ä—ã
        strong_pairs = self.find_strong_pairs(price_matrix, correlation_matrix)
        
        if not strong_pairs:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏—Ö –ø–∞—Ä")
            print("   –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å min_correlation –∏–ª–∏ —É–≤–µ–ª–∏—á–∏—Ç—å min_common_days")
            return
        
        # 5. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø–∞—Ä
        pairs_df = self.analyze_pair_characteristics(strong_pairs, metadata)
        
        # 6. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –¥–ª—è —Ç–æ–ø-–ø–∞—Ä
        pairs_with_coint = self.calculate_cointegration(price_matrix, pairs_df, top_n=20)
        
        # 7. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        files = self.save_results(pairs_with_coint, correlation_matrix, price_matrix)
        
        # 8. –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
        self.print_summary(pairs_with_coint, files)

def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞"""
    try:
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å
        calculator = CorrelationCalculator(
            min_correlation=0.7,    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
            min_common_days=100     # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –æ–±—â–∏—Ö –¥–Ω–µ–π
        )
        calculator.run()
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
